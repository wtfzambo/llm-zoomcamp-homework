{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b62e6-12ab-4951-aca7-d910b907b511",
   "metadata": {
    "id": "212b62e6-12ab-4951-aca7-d910b907b511"
   },
   "outputs": [],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XCreq7R22e4T",
   "metadata": {
    "collapsed": true,
    "id": "XCreq7R22e4T",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WkGqzTyp3Bkq",
   "metadata": {
    "id": "WkGqzTyp3Bkq"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gA76vstX2UWj",
   "metadata": {
    "id": "gA76vstX2UWj"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8lUuyiQ22Wna",
   "metadata": {
    "id": "8lUuyiQ22Wna"
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVxFm6GR2ayO",
   "metadata": {
    "id": "IVxFm6GR2ayO"
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UoEjEMN82kNQ",
   "metadata": {
    "id": "UoEjEMN82kNQ"
   },
   "source": [
    "# video 2.4 - phi 3 mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bO3NKcfm2mY5",
   "metadata": {
    "id": "bO3NKcfm2mY5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xxlg5fTz3GZf",
   "metadata": {
    "id": "xxlg5fTz3GZf"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ABmciWdn3INx",
   "metadata": {
    "id": "ABmciWdn3INx"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bSOF14y-3Kpz",
   "metadata": {
    "id": "bSOF14y-3Kpz"
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KNHhZ_NA3NK7",
   "metadata": {
    "id": "KNHhZ_NA3NK7"
   },
   "outputs": [],
   "source": [
    "rag(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AB8mwHNf-l-r",
   "metadata": {
    "id": "AB8mwHNf-l-r"
   },
   "source": [
    "# video 2.5 - mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jh-kUCBbAd8f",
   "metadata": {
    "id": "Jh-kUCBbAd8f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PFpLS6Q63Vll",
   "metadata": {
    "id": "PFpLS6Q63Vll"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5Q4gEZrBO5a",
   "metadata": {
    "id": "t5Q4gEZrBO5a"
   },
   "outputs": [],
   "source": [
    "login(token=userdata.get('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qhHykY9wEBKS",
   "metadata": {
    "id": "qhHykY9wEBKS"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0JJV_GohD8_W",
   "metadata": {
    "id": "0JJV_GohD8_W"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhP4W8IYAl4Y",
   "metadata": {
    "id": "xhP4W8IYAl4Y"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "### ran out of GPU ram :(((("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6CqUfRZBABex",
   "metadata": {
    "id": "6CqUfRZBABex"
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"{doc['question']}\\n{doc['text']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = pipe(prompt, max_length=500, temperature=0.7, top_p=0.95, num_return_sequences=1)\n",
    "    response_final = response[0]['generated_text']\n",
    "    return response_final[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74274b-bdfb-4f7f-ae33-3fd284434a6d",
   "metadata": {},
   "source": [
    "# video 2.7 - ollama, running llms on a CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7751c5c-34c1-4909-94ee-219841d04b41",
   "metadata": {},
   "source": [
    "first install ollama \n",
    "\n",
    "```sh\n",
    "brew install --cask ollama\n",
    "```\n",
    "\n",
    "then start server\n",
    "\n",
    "```sh\n",
    "ollama start\n",
    "```\n",
    "\n",
    "then pull a model\n",
    "\n",
    "```sh\n",
    "ollama pull phi3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f017b29-855f-4606-8183-8e835177512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4af0a90-7e1c-48d4-b327-4633b92ca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3704f966-9e0c-4d37-a766-1060241b527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    res = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    return res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d441406c-ed9d-44d4-b219-e232d976062a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a test.\\n\\n\\nTo execute and verify the functionality of your system, run an automated process which includes: \\n\\n- Initiating with \"This is a test.\" to simulate typical operations.\\n\\n- Monitoring for expected outcomes such as correct text display without errors.\\n\\n- Logging results into a testing report that details execution and observations, ensuring transparency in the automated process.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('write that this is a test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07d9a581-6aab-42a0-9cc0-952d7b526b84",
   "metadata": {},
   "source": [
    "Let's now run it in docker\n",
    "\n",
    "```sh\n",
    "docker run -it \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "```\n",
    "\n",
    "remember to pull the model again inside the container, otherwise it'd have no model to use!\n",
    "(this is why we mount a volume, to persist the downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c31ae1e-6834-4efe-a6b8-81fd067f9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288bb63e-c252-4a73-96a4-c26f1db77a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a test.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('write the following sentence and nothing else: \"this is a test\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533fdb5-4d2c-45cd-82fd-6d1befd9edbb",
   "metadata": {},
   "source": [
    "# video 2.8 - ollama and phi3 + elasticsearch in docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6343dcc-c178-4358-9c2c-0e7b16bd6f18",
   "metadata": {},
   "source": [
    "First we start by creating a docker-compose file, running ollama and elastic search:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  elasticsearch:\n",
    "    image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
    "    container_name: elasticsearch\n",
    "    environment:\n",
    "      - discovery.type=single-node\n",
    "      - xpack.security.enabled=false\n",
    "    ports:\n",
    "      - '9200:9200'\n",
    "      - '9300:9300'\n",
    "\n",
    "  ollama:\n",
    "    image: ollama/ollama\n",
    "    container_name: ollama\n",
    "    volumes:\n",
    "      - ollama:/root/.ollama\n",
    "    ports:\n",
    "      - '11434:11434'    \n",
    "\n",
    "volumes:\n",
    "  ollama:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9214c-c55e-4f10-8187-532ebd1f4b2e",
   "metadata": {},
   "source": [
    "Now we run the same thing from module 1 but with Ollama + ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d768f3e-7e42-458c-81b5-b4e66a384a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b50718f-e9da-42b7-8570-4275b82558e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9ce804b-ae8f-4887-90f9-420511a37dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1daa1239-1e0c-40f7-a51f-39936b158df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'course-questions'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7253fa5-cd05-43a7-870c-52ad77277753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8be85ae6-4cea-42de-a063-f8463144c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dce444e-758c-422b-9adc-b8d720c7b62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 948/948 [00:08<00:00, 117.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, body=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb96713-3e1f-4b70-9d54-e3d21ec01bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c5c1811-2d85-424e-b95f-3214713dd5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03a106da-b0f4-440a-83ce-8060ea07e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb9eafb8-aebb-4e85-acbf-c19726828570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, you are still eligible to join after discovering your course of choice in our Data Engineering Bootcamp even if it has already started since there will be a deadline for final projects submission and we keep all materials available post-course completion as stated in question 2 answer section from the FAQ database. However, consider reviewing prerequisites, syllabus (question3), understanding course structure beforehand can greatly improve your learning experience which might help you get a head start with homework submissions even without receiving an initial confirmation email(Question1). While registration isn't compulsory and doesn't serve as formal acceptance it may provide certain benefits like direct access to FAQ (question4) and other resources, making the learning process smoother. You can begin your journey by installing necessary software dependencies immediately upon discovering about this course(Question2). Lastly note that you still have time before starting date of Bootcamp as per answer section 3 from FAQ database even if it appears to already be started but remember not to leave everything for the last minute (question1), keeping in mind project deadlines.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'I just disovered the course. Can I still join it?'\n",
    "rag(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be407037-0ed4-46a9-8d85-312242618fa5",
   "metadata": {},
   "source": [
    "# video 2.9 UI for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b4a61b9-1a10-4118-8249-a670d2215e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.36.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from streamlit) (2.2.2)\n",
      "Collecting pillow<11,>=7.1.0 (from streamlit)\n",
      "  Downloading pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.27.2-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from streamlit) (2.32.2)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from streamlit) (4.11.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from streamlit) (6.4.1)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema-4.22.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.18.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/llm_zoomcamp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading protobuf-5.27.2-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-macosx_11_0_arm64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.18.1-cp310-cp310-macosx_11_0_arm64.whl (322 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toolz, toml, tenacity, smmap, rpds-py, pyarrow, protobuf, pillow, mdurl, MarkupSafe, click, cachetools, blinker, referencing, markdown-it-py, jinja2, gitdb, rich, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed MarkupSafe-2.1.5 altair-5.3.0 blinker-1.8.2 cachetools-5.3.3 click-8.1.7 gitdb-4.0.11 gitpython-3.1.43 jinja2-3.1.4 jsonschema-4.22.0 jsonschema-specifications-2023.12.1 markdown-it-py-3.0.0 mdurl-0.1.2 pillow-10.4.0 protobuf-5.27.2 pyarrow-16.1.0 pydeck-0.9.1 referencing-0.35.1 rich-13.7.1 rpds-py-0.18.1 smmap-5.0.1 streamlit-1.36.0 tenacity-8.4.2 toml-0.10.2 toolz-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64997a4b-16a0-44e8-b792-4fbae8546fe2",
   "metadata": {},
   "source": [
    "code in `streamlit_app.py`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:llm_zoomcamp]",
   "language": "python",
   "name": "conda-env-llm_zoomcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
