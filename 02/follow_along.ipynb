{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212b62e6-12ab-4951-aca7-d910b907b511",
      "metadata": {
        "id": "212b62e6-12ab-4951-aca7-d910b907b511"
      },
      "outputs": [],
      "source": [
        "!rm -f minsearch.py\n",
        "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "XCreq7R22e4T",
        "collapsed": true
      },
      "id": "XCreq7R22e4T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WkGqzTyp3Bkq"
      },
      "id": "WkGqzTyp3Bkq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import minsearch\n",
        "\n",
        "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
        "docs_response = requests.get(docs_url)\n",
        "documents_raw = docs_response.json()\n",
        "\n",
        "documents = []\n",
        "\n",
        "for course in documents_raw:\n",
        "    course_name = course['course']\n",
        "\n",
        "    for doc in course['documents']:\n",
        "        doc['course'] = course_name\n",
        "        documents.append(doc)\n",
        "\n",
        "index = minsearch.Index(\n",
        "    text_fields=[\"question\", \"text\", \"section\"],\n",
        "    keyword_fields=[\"course\"]\n",
        ")\n",
        "\n",
        "index.fit(documents)"
      ],
      "metadata": {
        "id": "gA76vstX2UWj"
      },
      "id": "gA76vstX2UWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query):\n",
        "    boost = {'question': 3.0, 'section': 0.5}\n",
        "\n",
        "    results = index.search(\n",
        "        query=query,\n",
        "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
        "        boost_dict=boost,\n",
        "        num_results=5\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8lUuyiQ22Wna"
      },
      "id": "8lUuyiQ22Wna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(query):\n",
        "    search_results = search(query)\n",
        "    prompt = build_prompt(query, search_results)\n",
        "    answer = llm(prompt)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "IVxFm6GR2ayO"
      },
      "id": "IVxFm6GR2ayO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# video 2.4 - phi 3 mini"
      ],
      "metadata": {
        "id": "UoEjEMN82kNQ"
      },
      "id": "UoEjEMN82kNQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)"
      ],
      "metadata": {
        "id": "bO3NKcfm2mY5"
      },
      "id": "bO3NKcfm2mY5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
      ],
      "metadata": {
        "id": "xxlg5fTz3GZf"
      },
      "id": "xxlg5fTz3GZf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "ABmciWdn3INx"
      },
      "id": "ABmciWdn3INx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(query, search_results):\n",
        "    prompt_template = \"\"\"\n",
        "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
        "Use only the facts from the CONTEXT when answering the QUESTION.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "    context = \"\"\n",
        "\n",
        "    for doc in search_results:\n",
        "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
        "\n",
        "    prompt = prompt_template.format(question=query, context=context).strip()\n",
        "    return prompt\n",
        "\n",
        "def llm(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 500,\n",
        "        \"return_full_text\": False,\n",
        "        \"temperature\": 0.0,\n",
        "        \"do_sample\": False,\n",
        "    }\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    return output[0]['generated_text'].strip()"
      ],
      "metadata": {
        "id": "bSOF14y-3Kpz"
      },
      "id": "bSOF14y-3Kpz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag(\"I just discovered the course. Can I still join it?\")"
      ],
      "metadata": {
        "id": "KNHhZ_NA3NK7"
      },
      "id": "KNHhZ_NA3NK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# video 2.5 - mistral-7B"
      ],
      "metadata": {
        "id": "AB8mwHNf-l-r"
      },
      "id": "AB8mwHNf-l-r"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "Jh-kUCBbAd8f"
      },
      "id": "Jh-kUCBbAd8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "torch.random.manual_seed(0)"
      ],
      "metadata": {
        "id": "PFpLS6Q63Vll"
      },
      "id": "PFpLS6Q63Vll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "t5Q4gEZrBO5a"
      },
      "id": "t5Q4gEZrBO5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "qhHykY9wEBKS"
      },
      "id": "qhHykY9wEBKS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")"
      ],
      "metadata": {
        "id": "0JJV_GohD8_W"
      },
      "id": "0JJV_GohD8_W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "### ran out of GPU ram :(((("
      ],
      "metadata": {
        "id": "xhP4W8IYAl4Y"
      },
      "id": "xhP4W8IYAl4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(query, search_results):\n",
        "    prompt_template = \"\"\"\n",
        "QUESTION: {question}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\".strip()\n",
        "\n",
        "    context = \"\"\n",
        "\n",
        "    for doc in search_results:\n",
        "        context = context + f\"{doc['question']}\\n{doc['text']}\\n\\n\"\n",
        "\n",
        "    prompt = prompt_template.format(question=query, context=context).strip()\n",
        "    return prompt\n",
        "\n",
        "def llm(prompt):\n",
        "    response = pipe(prompt, max_length=500, temperature=0.7, top_p=0.95, num_return_sequences=1)\n",
        "    response_final = response[0]['generated_text']\n",
        "    return response_final[len(prompt):].strip()"
      ],
      "metadata": {
        "id": "6CqUfRZBABex"
      },
      "id": "6CqUfRZBABex",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}